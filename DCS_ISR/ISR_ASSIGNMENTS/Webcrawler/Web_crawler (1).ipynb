{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPUc6966ulkc",
    "outputId": "d668611e-e35f-4d38-d9f1-a7396a4482f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page: https://books.toscrape.com/catalogue/page-1.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-2.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-3.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-4.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-5.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-6.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-7.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-8.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-9.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-10.html\n",
      "Saved 200 products to books.csv\n"
     ]
    }
   ],
   "source": [
    "# simple_crawler.py\n",
    "# Install all dependencies\n",
    "!pip install requests beautifulsoup4 --quiet\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib import robotparser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://books.toscrape.com/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"MyCrawler/1.0 (+https://example.com/contact) - educational use\"\n",
    "}\n",
    "\n",
    "# polite crawler: check robots.txt\n",
    "def can_fetch(url, user_agent=HEADERS[\"User-Agent\"]):\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    robots_url = urljoin(url, \"/robots.txt\")\n",
    "    rp.set_url(robots_url)\n",
    "    try:\n",
    "        rp.read()\n",
    "    except Exception:\n",
    "        # if robots.txt can't be fetched, default to false to be safe or True if you prefer\n",
    "        return False\n",
    "    return rp.can_fetch(user_agent, url)\n",
    "\n",
    "def get_soup(url):\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "def parse_product_card(card, base_url):\n",
    "    # Example for books.toscrape structure — change selectors for your target site\n",
    "    title_tag = card.select_one(\"h3 a\")\n",
    "    title = title_tag[\"title\"].strip()\n",
    "    relative_link = title_tag[\"href\"]\n",
    "    product_url = urljoin(base_url, relative_link)\n",
    "\n",
    "    price = card.select_one(\".price_color\").get_text(strip=True)\n",
    "    availability = card.select_one(\".availability\").get_text(strip=True)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"price\": price,\n",
    "        \"availability\": availability,\n",
    "        \"product_url\": product_url\n",
    "    }\n",
    "\n",
    "def crawl(start_url, max_pages=5, delay=1.0, output_csv=\"products.csv\"):\n",
    "    # Respect robots.txt for the start URL domain\n",
    "    parsed = urlparse(start_url)\n",
    "    domain_root = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
    "    if not can_fetch(domain_root):\n",
    "        raise SystemExit(f\"Robots.txt disallows crawling {domain_root} for this user-agent.\")\n",
    "\n",
    "    products = []\n",
    "    next_page = start_url\n",
    "    pages_crawled = 0\n",
    "\n",
    "    while next_page and pages_crawled < max_pages:\n",
    "        print(f\"Crawling page: {next_page}\")\n",
    "        soup = get_soup(next_page)\n",
    "\n",
    "        # Find all product cards — change this selector to match the site\n",
    "        cards = soup.select(\".product_pod\")\n",
    "        for card in cards:\n",
    "            try:\n",
    "                prod = parse_product_card(card, domain_root)\n",
    "                products.append(prod)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse product card:\", e)\n",
    "\n",
    "        # Find \"next\" link (example specific to books.toscrape)\n",
    "        next_tag = soup.select_one(\".next a\")\n",
    "        if next_tag:\n",
    "            next_page = urljoin(next_page, next_tag[\"href\"])\n",
    "        else:\n",
    "            next_page = None\n",
    "\n",
    "        pages_crawled += 1\n",
    "        time.sleep(delay)  # polite delay\n",
    "\n",
    "    # Save results\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"price\", \"availability\", \"product_url\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(products)\n",
    "\n",
    "    print(f\"Saved {len(products)} products to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # start crawling from catalog page\n",
    "    start = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "    crawl(start, max_pages=10, delay=1.0, output_csv=\"books.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VbIQcixhvmwy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 100 items\n"
     ]
    }
   ],
   "source": [
    "import requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "data = []\n",
    "\n",
    "for _ in range(5):   # number of pages\n",
    "    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "    for b in soup.select(\".product_pod\"):\n",
    "        title = b.h3.a[\"title\"]\n",
    "        price = b.select_one(\".price_color\").text\n",
    "        avail = b.select_one(\".availability\").text.strip()\n",
    "        link = \"https://books.toscrape.com/catalogue/\" + b.h3.a[\"href\"]\n",
    "        data.append([title, price, avail, link])\n",
    "\n",
    "    nxt = soup.select_one(\".next a\")\n",
    "    if not nxt: break\n",
    "    url = \"https://books.toscrape.com/catalogue/\" + nxt[\"href\"]\n",
    "\n",
    "with open(\"products.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.writer(f).writerows([(\"Title\",\"Price\",\"Availability\",\"Link\")] + data)\n",
    "\n",
    "print(\"Saved:\", len(data), \"items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
