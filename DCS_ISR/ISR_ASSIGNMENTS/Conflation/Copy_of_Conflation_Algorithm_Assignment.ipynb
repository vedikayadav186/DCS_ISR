{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WbNGfD8va2bs",
    "outputId": "9d475a73-ecfb-47e4-ce2e-ba2ba17b0f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Natural Language Processing is an important field of Computer Science. \n",
      "It focuses on enabling computers to understand human language. \n",
      "People speak and write in different ways, so words often appear in various forms. \n",
      "To analyze text effectively, these word variations need to be reduced to a common base form. \n",
      "This process of reducing related words to a single representative form is known as conflation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "After Stemming:\n",
      " natur languag process import field comput scienc focus enabl comput understand human languag peopl speak write differ way word often appear variou form analyz text effect word variat need reduc common base form process reduc relat word singl repres form known conflat\n",
      "--------------------------------------------------------------------------------\n",
      "After Lemmatization:\n",
      " natural language processing important field computer science focus enabling computer understand human language people speak write different way word often appear various form analyze text effectively word variation need reduced common base form process reducing related word single representative form known conflation\n",
      "--------------------------------------------------------------------------------\n",
      "Document Representative (Word Frequency):\n",
      "word: 3\n",
      "form: 3\n",
      "languag: 2\n",
      "process: 2\n",
      "comput: 2\n",
      "reduc: 2\n",
      "natur: 1\n",
      "import: 1\n",
      "field: 1\n",
      "scienc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Conflation Algorithm: Generate Document Representative of a Text File\n",
    "# Using Stemming and Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "# Download resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')   # <-- ADD THIS LINE\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Step 1: Read input text file\n",
    "filename = \"Conflation.txt\"   # <-- use your own text file\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Original Text:\\n\", text)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Step 3: Remove punctuation and stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "# Step 4: Apply Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Step 5: Apply Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Step 6: Display results\n",
    "print(\"After Stemming:\\n\", ' '.join(stemmed_words))\n",
    "print(\"-\" * 80)\n",
    "print(\"After Lemmatization:\\n\", ' '.join(lemmatized_words))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Step 7: Create Document Representative (word frequency)\n",
    "from collections import Counter\n",
    "freq = Counter(stemmed_words)\n",
    "print(\"Document Representative (Word Frequency):\")\n",
    "for word, count in freq.most_common(10):\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ftuRAWXARlOi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:\n",
      " natur languag process import field comput scienc focus enabl comput understand human languag peopl speak write differ way word often appear variou form analyz text effect word variat need reduc common base form process reduc relat word singl repres form known conflat\n",
      "\n",
      "Lemmatization:\n",
      " natural language processing important field computer science focus enabling computer understand human language people speak write different way word often appear various form analyze text effectively word variation need reduced common base form process reducing related word single representative form known conflation\n",
      "\n",
      "Document Representative (Top Words):\n",
      "word : 3\n",
      "form : 3\n",
      "languag : 2\n",
      "process : 2\n",
      "comput : 2\n",
      "reduc : 2\n",
      "natur : 1\n",
      "import : 1\n",
      "field : 1\n",
      "scienc : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Download resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = open(\"Conflation.txt\", \"r\", encoding=\"utf-8\").read().lower()\n",
    "\n",
    "# Tokenize and remove stopwords + punctuation\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "words = [w for w in word_tokenize(text) if w.isalpha() and w not in stop]\n",
    "\n",
    "# Stemming\n",
    "stem = PorterStemmer()\n",
    "stemmed = [stem.stem(w) for w in words]\n",
    "\n",
    "# Lemmatization\n",
    "lemma = WordNetLemmatizer()\n",
    "lemmatized = [lemma.lemmatize(w) for w in words]\n",
    "\n",
    "# Output\n",
    "print(\"Stemming:\\n\", \" \".join(stemmed))\n",
    "print(\"\\nLemmatization:\\n\", \" \".join(lemmatized))\n",
    "print(\"\\nDocument Representative (Top Words):\")\n",
    "for w, c in Counter(stemmed).most_common(10):\n",
    "    print(w, \":\", c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vedik\\appdata\\roaming\\python\\python312\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\vedik\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vedik\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vedik\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vedik\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\vedik\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Original Text:\n",
      " Natural Language Processing is an important field of Computer Science. \n",
      "It focuses on enabling computers to understand human language. \n",
      "People speak and write in different ways, so words often appear in various forms. \n",
      "To analyze text effectively, these word variations need to be reduced to a common base form. \n",
      "This process of reducing related words to a single representative form is known as conflation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "After Stemming:\n",
      " natur languag process import field comput scienc focus enabl comput understand human languag peopl speak write differ way word often appear variou form analyz text effect word variat need reduc common base form process reduc relat word singl repres form known conflat\n",
      "--------------------------------------------------------------------------------\n",
      "After Lemmatization:\n",
      " natural language processing important field computer science focus enabling computer understand human language people speak write different way word often appear various form analyze text effectively word variation need reduced common base form process reducing related word single representative form known conflation\n",
      "--------------------------------------------------------------------------------\n",
      "Document Representative (Word Frequency):\n",
      "word: 3\n",
      "form: 3\n",
      "languag: 2\n",
      "process: 2\n",
      "comput: 2\n",
      "reduc: 2\n",
      "natur: 1\n",
      "import: 1\n",
      "field: 1\n",
      "scienc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vedik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Conflation Algorithm: Generate Document Representative of a Text File\n",
    "# Using Stemming and Lemmatization\n",
    "# ==============================================\n",
    "\n",
    "# ====== Install Required Libraries ======\n",
    "!pip install nltk \n",
    "\n",
    "# ====== Import Libraries ======\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# ====== Download NLTK Resources ======\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')   # For tokenization tables\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ====== Step 1: Read Input Text File ======\n",
    "filename = \"Conflation.txt\"   # <-- use your own text file\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Original Text:\\n\", text)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ====== Step 2: Tokenization ======\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# ====== Step 3: Remove Punctuation and Stopwords ======\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "# ====== Step 4: Apply Stemming ======\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# ====== Step 5: Apply Lemmatization ======\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# ====== Step 6: Display Results ======\n",
    "print(\"After Stemming:\\n\", ' '.join(stemmed_words))\n",
    "print(\"-\" * 80)\n",
    "print(\"After Lemmatization:\\n\", ' '.join(lemmatized_words))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ====== Step 7: Create Document Representative (Word Frequency) ======\n",
    "freq = Counter(stemmed_words)\n",
    "print(\"Document Representative (Word Frequency):\")\n",
    "for word, count in freq.most_common(10):\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
